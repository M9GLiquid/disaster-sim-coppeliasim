{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ----------------------\n",
        "# Imports\n",
        "# ----------------------"
      ],
      "metadata": {
        "id": "Km4SV0o_uwZO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE42p9f_uNka"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm  # For MobileNetV3/EfficientNet-Lite\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ----------------------\n",
        "# Dataset Loader\n",
        "# ----------------------"
      ],
      "metadata": {
        "id": "cxVq1OJkuvQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DroneBatchDataset(Dataset):\n",
        "    def __init__(self, npz_folder, sequence_length=5, transform=None):\n",
        "        self.files = [os.path.join(npz_folder, f) for f in os.listdir(npz_folder) if f.endswith('.npy')]\n",
        "        self.sequence_length = sequence_length\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        for file in self.files:\n",
        "            data = np.load(file)\n",
        "            depths = data['depths']\n",
        "            actions = data['actions']\n",
        "            victim_dirs = data['victim_dirs']\n",
        "            for i in range(len(depths) - sequence_length + 1):\n",
        "                self.samples.append((file, i))\n",
        "        self.data_cache = {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file, start_idx = self.samples[idx]\n",
        "        if file not in self.data_cache:\n",
        "            data = np.load(file)\n",
        "            self.data_cache[file] = {\n",
        "                'depths': data['depths'],\n",
        "                'actions': data['actions'],\n",
        "                'victim_dirs': data['victim_dirs']\n",
        "            }\n",
        "        d = self.data_cache[file]\n",
        "        depths_seq = d['depths'][start_idx:start_idx+self.sequence_length]\n",
        "        actions_seq = d['actions'][start_idx:start_idx+self.sequence_length]\n",
        "        victim_dirs_seq = d['victim_dirs'][start_idx:start_idx+self.sequence_length]\n",
        "        depths_seq = np.expand_dims(depths_seq, 1)\n",
        "        if self.transform:\n",
        "            depths_seq = self.transform(torch.from_numpy(depths_seq).float())\n",
        "        return (\n",
        "            torch.from_numpy(depths_seq).float(),\n",
        "            torch.from_numpy(victim_dirs_seq).float(),\n",
        "            torch.from_numpy(actions_seq).long()\n",
        "        )"
      ],
      "metadata": {
        "id": "xc1hr4HVuPc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ----------------------\n",
        "# Model Definition\n",
        "# ----------------------"
      ],
      "metadata": {
        "id": "WmnL92upuswO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLSTMCell(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv = nn.Conv2d(input_dim + hidden_dim, 4 * hidden_dim, kernel_size, padding=padding, bias=bias)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        combined = torch.cat([x, h], dim=1)\n",
        "        conv_output = self.conv(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_dim, dim=1)\n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "        c_next = f * c + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "        return h_next, c_next\n",
        "\n",
        "class SimpleConvLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size=3, bias=True):\n",
        "        super().__init__()\n",
        "        self.cell = ConvLSTMCell(input_dim, hidden_dim, kernel_size, bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, seq_len, C, H, W = x.size()\n",
        "        h, c = (torch.zeros(batch, self.cell.hidden_dim, H, W, device=x.device),\n",
        "                torch.zeros(batch, self.cell.hidden_dim, H, W, device=x.device))\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            h, c = self.cell(x[:, t], h, c)\n",
        "            outputs.append(h)\n",
        "        return torch.stack(outputs, dim=1)\n",
        "\n",
        "class DroneActionNet(nn.Module):\n",
        "    def __init__(self, num_actions=9, victim_dir_dim=4, backbone='mobilenetv3_small', convlstm_hidden=32):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(backbone, pretrained=True, features_only=True, in_chans=1)\n",
        "        backbone_out_ch = self.backbone.feature_info[-1]['num_chs']\n",
        "        self.convlstm = SimpleConvLSTM(backbone_out_ch, convlstm_hidden)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.victim_fc = nn.Linear(victim_dir_dim, 16)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(convlstm_hidden + 16, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, victim_dirs):\n",
        "        batch, seq, _, H, W = x.size()\n",
        "        x = x.view(batch * seq, 1, H, W)\n",
        "        feats = self.backbone(x)[-1]\n",
        "        _, C, h, w = feats.size()\n",
        "        feats = feats.view(batch, seq, C, h, w)\n",
        "        convlstm_out = self.convlstm(feats)\n",
        "        pooled = self.pool(convlstm_out[:, -1]).view(batch, -1)\n",
        "        victim_emb = self.victim_fc(victim_dirs[:, -1])\n",
        "        out = torch.cat([pooled, victim_emb], dim=1)\n",
        "        logits = self.fc(out)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "fis3LSeYuUBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ----------------------\n",
        "# Training and Evaluation\n",
        "# ----------------------"
      ],
      "metadata": {
        "id": "TokAF1VquqA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss, total_correct, total_samples = 0, 0, 0\n",
        "    for x, victim_dirs, y in loader:\n",
        "        x, victim_dirs, y = x.to(device), victim_dirs.to(device), y[:, -1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x, victim_dirs)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        total_correct += (logits.argmax(1) == y).sum().item()\n",
        "        total_samples += x.size(0)\n",
        "    return total_loss / total_samples, total_correct / total_samples\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, victim_dirs, y in loader:\n",
        "            x, victim_dirs, y = x.to(device), victim_dirs.to(device), y[:, -1].to(device)\n",
        "            logits = model(x, victim_dirs)\n",
        "            preds = logits.argmax(1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    return acc, f1, cm\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "APml6lT7uYpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ----------------------\n",
        "# Quantization\n",
        "# ----------------------"
      ],
      "metadata": {
        "id": "tiP5xoe8unPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_model(model):\n",
        "    model.eval()\n",
        "    model.cpu()\n",
        "    quantized_model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
        "    return quantized_model"
      ],
      "metadata": {
        "id": "0TpKbJUBuhAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User: Set these paths and hyperparameters\n",
        "    train_dir = '/path/to/train'\n",
        "    val_dir = '/path/to/val'\n",
        "    sequence_length = 5\n",
        "    batch_size = 8\n",
        "    num_epochs = 10\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    train_dataset = DroneBatchDataset(train_dir, sequence_length=sequence_length)\n",
        "    val_dataset = DroneBatchDataset(val_dir, sequence_length=sequence_length)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "    model = DroneActionNet().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_acc, val_f1, val_cm = evaluate(model, val_loader, device)\n",
        "        print(f\"Epoch {epoch}: Train Loss {train_loss:.4f}, Train Acc {train_acc:.4f}, Val Acc {val_acc:.4f}, Val F1 {val_f1:.4f}\")\n",
        "\n",
        "    class_names = ['Right','Left','Forward','Backward','Up','Down','TurnL','TurnR','Hover']\n",
        "    val_acc, val_f1, val_cm = evaluate(model, val_loader, device)\n",
        "    plot_confusion_matrix(val_cm, class_names)\n",
        "\n",
        "    # Quantize and evaluate\n",
        "    quantized_model = quantize_model(model)\n",
        "    val_acc, val_f1, val_cm = evaluate(quantized_model, val_loader, device)\n",
        "    print(f\"Quantized Model: Val Acc {val_acc:.4f}, Val F1 {val_f1:.4f}\")"
      ],
      "metadata": {
        "id": "63i3id71uhcx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
# -*- coding: utf-8 -*-
"""[Training 2.0] Ai-for-robotics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IUHpL0qSVKL8vFCHA-6EWdJKDvipYgas

# Notebook

This notebook contains the core components for training, validating, and testing the ConvLSTM-based DroneActionNet model on depth-image sequences.  
Cells are grouped into explanatory **Markdown** and **Code** sections for clarity.

## 1. Imports & Setup

Load libraries, suppress warnings, and configure plotting.
"""

import os
import warnings
import numpy as np
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import timm
import matplotlib.pyplot as plt

# Suppress timm warnings
warnings.filterwarnings("ignore", message="Unexpected ke")

if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print("CUDA cache cleared.")
else:
    print("CUDA not available. No cache to clear.")

"""## 2. Hyperparameters & Device Configuration

Define dataset paths, including commented-out alternative test directory for easy switching, along with training parameters.
"""

# Configure paths for local environment
data_base_dir = '/content/drive/MyDrive/AI for Robotics/Disaster Simulator/depth_dataset'
train_dir = os.path.join(data_base_dir, 'train')
val_dir = os.path.join(data_base_dir, 'val')
test_dir = os.path.join(data_base_dir, 'test')

base_model_save_dir = '/content/drive/MyDrive/AI for Robotics/models'

# Create the directories if they don't exist
os.makedirs(base_model_save_dir, exist_ok=True)
os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

# Check if dataset directories have data
for split, folder in [('Train', train_dir), ('Val', val_dir), ('Test', test_dir)]:
    if not os.path.exists(folder):
        print(f"Warning: {split} directory does not exist: {folder}")
    elif not any(f.endswith('.npz') for f in os.listdir(folder)):
        print(f"Warning: No .npz files found in {split} directory: {folder}")

sequence_length = 10   # number of frames per sequence
batch_size      = 32   # samples per batch
num_epochs      = 100  # training epochs
learning_rate   = 1e-4

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Map class indices to action names (customize as needed)
action_names = ['Right', 'Left', 'Forward', 'Backward', 'Up', 'Down', 'Left Yaw', 'Right Yaw', 'Hover']

"""## 3. Dataset Loader

`DroneBatchDataset` loads `.npz` files, extracts depth sequences, victim embeddings, and action labels.
"""

class DroneBatchDataset(Dataset):
    """
    Loads depth image sequences and labels from NPZ files.
    Each .npz file is treated as a separate episode.
    Each sample is a contiguous sequence of length `sequence_length` within a file.
    Loads data for a sequence on demand from the NPZ file.
    """
    def __init__(self, folder, sequence_length=5, transform=None, include_actions=True):
        self.sequence_length = sequence_length
        self.transform = transform
        self.include_actions = include_actions
        self.files = sorted([os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.npz')])
        self.samples = []  # Stores (file_index, sequence_start_frame_index) tuples

        # Process each file to determine valid sequence starts
        for file_idx, file_path in enumerate(self.files):
            try:
                # Load only header information to get number of frames if possible
                # Or, if necessary, load a small part to get the size
                with np.load(file_path) as data:
                     if 'depths' in data:
                        num_frames = data['depths'].shape[0]
                        # Add valid sequence start indices for this file
                        for start_frame_idx in range(num_frames - self.sequence_length + 1):
                            self.samples.append((file_idx, start_frame_idx))
                     else:
                        print(f"Warning: 'depths' missing in file: {file_path}. Skipping file.")
            except Exception as e:
                print(f"Error loading file {file_path}: {e}. Skipping file.")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        file_idx, sequence_start_frame_index = self.samples[idx]
        file_path = self.files[file_idx]

        depths = []
        victim_dirs = []
        actions = [] # Only needed if include_actions is True
        distances = []

        try:
            with np.load(file_path) as data:
                num_frames_in_file = data['depths'].shape[0] # Assuming depths is always present

                for i in range(self.sequence_length):
                    frame_idx = sequence_start_frame_index + i

                    if frame_idx < num_frames_in_file: # Safety check
                        if 'depths' in data:
                            depths.append(data['depths'][frame_idx])
                        if 'victim_dirs' in data:
                            victim_dirs.append(data['victim_dirs'][frame_idx])
                        if self.include_actions and 'actions' in data:
                            actions.append(data['actions'][frame_idx])
                        if 'distances' in data:
                             distances.append(data['distances'][frame_idx])


            # Data validation after loading sequence
            if len(depths) < self.sequence_length or len(victim_dirs) < self.sequence_length:
                 raise ValueError(f"Sample {idx}: Not enough frames to construct sequence (depths or victim_dirs missing or incomplete)")

            depths = torch.from_numpy(np.stack(depths)).unsqueeze(1).float() # Add channel dim
            vic = torch.from_numpy(np.stack(victim_dirs)).float()

            if len(distances) == self.sequence_length:
                 vic = np.concatenate([np.stack(victim_dirs), np.expand_dims(np.stack(distances), axis=1)], axis=1)
                 vic = torch.from_numpy(vic).float()

            if self.include_actions and len(actions) == self.sequence_length:
                actions = torch.from_numpy(np.stack(actions)).long()
            elif self.include_actions:
                 # Handle case where actions are missing for this sequence
                 actions = None # Or raise an error depending on your needs
                 print(f"Warning: Actions missing for sequence starting at {sequence_start_frame_index} in file {file_path}")


            if self.transform:
                depths = self.transform(depths)

            if self.include_actions:
                return depths, vic, actions
            else:
                return depths, vic

        except Exception as e:
            print(f"Error loading data for sample {idx} from file {file_path}: {e}. Skipping sample.")
            # Return None or raise an error, depending on how you want to handle missing data
            return None

"""## 4. ConvLSTM Implementation

Defines a single `ConvLSTMCell` and a wrapper for processing sequences.
"""

class ConvLSTMCell(nn.Module):
    def __init__(self, in_ch, hidden_ch, k=3):
        super().__init__()
        p = k // 2
        self.conv = nn.Conv2d(in_ch + hidden_ch, 4 * hidden_ch, k, padding=p)
        self.hidden_ch = hidden_ch
    def forward(self, x, h, c):
        combined = torch.cat([x, h], 1)
        gates = self.conv(combined)
        i, f, o, g = gates.chunk(4, 1)
        i, f, o = torch.sigmoid(i), torch.sigmoid(f), torch.sigmoid(o)
        g = torch.tanh(g)
        c = f * c + i * g
        h = o * torch.tanh(c)
        return h, c

class SimpleConvLSTM(nn.Module):
    def __init__(self, in_ch, hidden_ch, k=3):
        super().__init__()
        self.cell = ConvLSTMCell(in_ch, hidden_ch, k)
    def forward(self, x):
        B, T, C, H, W = x.shape
        h = torch.zeros(B, self.cell.hidden_ch, H, W, device=x.device)
        c = torch.zeros_like(h)
        outs = []
        for t in range(T):
            h, c = self.cell(x[:, t], h, c)
            outs.append(h)
        return torch.stack(outs, 1)

"""## 5. Model Definition

`DroneActionNet` uses a pretrained MobileNetV3 backbone, ConvLSTM temporal layer, and FC layers for classification.
"""

class DroneActionNet(nn.Module):
    def __init__(self, num_actions=9, vic_dim=4, backbone='mobilenetv3_small_100', hidden=32): # Changed vic_dim to 4
        super().__init__()
        self.backbone = timm.create_model(backbone, pretrained=True, features_only=True, in_chans=1)
        out_ch = self.backbone.feature_info[-1]['num_chs']
        self.lstm = SimpleConvLSTM(out_ch, hidden)
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.vfc = nn.Linear(vic_dim, 16)
        self.fc = nn.Sequential(
            nn.Linear(hidden + 16, 64),
            nn.ReLU(),
            nn.Linear(64, num_actions)
        )
    def forward(self, x, vic):
        B, T, _, H, W = x.shape
        x = x.view(B * T, 1, H, W)
        feat = self.backbone(x)[-1]
        feat = feat.view(B, T, feat.size(1), feat.size(2), feat.size(3))
        out = self.lstm(feat)
        img = self.pool(out[:, -1]).view(B, -1)
        # vic[:, -1] has shape (B, 4) because victim_dirs is 3D and distances is 1D
        vic_feat = self.vfc(vic[:, -1])
        return self.fc(torch.cat([img, vic_feat], 1))

"""## 6. Training & Evaluation Functions

Separate functions handle training and validation, with confusion matrix output in eval.
"""

def train_model(model, criterion, optimizer, train_loader, device):
    model.train()
    total, correct = 0, 0
    for x, v, y in train_loader:
        x, v, y = x.to(device), v.to(device), y[:, -1].to(device)
        optimizer.zero_grad()
        logits = model(x, v)
        loss = criterion(logits, y)
        loss.backward()
        optimizer.step()
        total += logits.size(0)
        correct += (logits.argmax(1) == y).sum().item()
    return correct / total

def eval_model(model, data_loader, device):
    model.eval()
    preds, labels = [], []
    with torch.no_grad():
        for x, v, y in data_loader:
            x, v, y = x.to(device), v.to(device), y[:, -1].to(device)
            out = model(x, v)
            preds += out.argmax(1).cpu().tolist()
            labels += y.cpu().tolist()
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average='macro')
    cm = confusion_matrix(labels, preds)
    return acc, f1, cm

"""## 7. Train

Runs training epochs, prints metrics, logs history, and plots validation confusion matrix each epoch.
"""

train_hist, val_hist = [], []
val_f1_hist = []

def train(checkpoint_path=None):
    train_ds = DroneBatchDataset(train_dir, sequence_length)
    val_ds = DroneBatchDataset(val_dir, sequence_length)

    train_ld = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)
    val_ld = DataLoader(val_ds, batch_size, num_workers=2, pin_memory=True)

    model = DroneActionNet().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    start_epoch = 0

    if checkpoint_path and os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch']
        global train_hist, val_hist, val_f1_hist
        train_hist = checkpoint.get('train_hist', [])
        val_hist = checkpoint.get('val_hist', [])
        val_f1_hist = checkpoint.get('val_f1_hist', [])
        print(f"Loaded checkpoint from {checkpoint_path}, resuming from epoch {start_epoch + 1}")
    else:
        print("No checkpoint found. Starting training from scratch.")

    # Start tracking total training time
    total_start_time = time.time()

    for epoch in range(start_epoch + 1, num_epochs + 1):
        # Start tracking time for the current epoch
        epoch_start_time = time.time()

        train_acc = train_model(model, criterion, optimizer, train_ld, device)
        val_acc, val_f1, val_cm = eval_model(model, val_ld, device)
        train_hist.append(train_acc)
        val_hist.append(val_acc)
        val_f1_hist.append(val_f1)

        # Calculate time taken for the current epoch
        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time

        print(f"Epoch {epoch} | Train Acc: {train_acc:.3f} | Val Acc: {val_acc:.3f} | Val F1: {val_f1:.3f} | Time: {epoch_duration:.2f} seconds")

        if epoch % 10 == 0 or epoch == num_epochs:
            # Create the full save path using os.path.join
            checkpoint_filename = f'drone_action_net_epoch_{epoch}.pth'
            full_save_path = os.path.join(base_model_save_dir, checkpoint_filename)

            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_hist': train_hist,
                'val_hist': val_hist,
                'val_f1_hist': val_f1_hist
            }, full_save_path)
            print(f"Checkpoint saved to {full_save_path}")

    # Calculate total training time
    total_end_time = time.time()
    total_duration = total_end_time - total_start_time

    print(f"\nTotal training time: {total_duration:.2f} seconds")

    return model

# --- Call the train function ---
saved_model_path = os.path.join(base_model_save_dir, 'drone_action_net_epoch_25.pth')
model = train(checkpoint_path=saved_model_path)